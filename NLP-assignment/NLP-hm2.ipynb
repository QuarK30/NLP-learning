{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-02, Probability Model A First Look: An Introduction of Language Model\n",
    "####  Assignment\n",
    "Review the course online programming code;\n",
    "\n",
    "Review the main questions;\n",
    "\n",
    "Using wikipedia corpus to build a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review the course online programming code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '/Users/quark30/Desktop/NLP.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.exists(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv(dataset,sep=';')\n",
    "dataframe[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = dataframe['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤掉所有的非文字和数字\n",
    "def token(string):\n",
    "    return ' '.join(re.findall('[\\w|\\d]+',string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token('此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/体验版内测，稳定版暂不受影响），以确保工程师可以集中全部精力进行系统优化工作。有人猜测这也是将精力主要用到MIUI 9的研发之中。\\nMIUI 8去年5月发布，距今已有一年有余，也是时候更新换代了。\\n当然，关于MIUI 9的确切信息，我们还是等待官方消息。\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = [token(str(a)) for a in all_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for a in all_articles:\n",
    "    text+=a\n",
    "print('length of text:{}'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "txt_from_reduce = reduce(lambda a1,a2:a1+a2,all_articles[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def cut(string): return list(jieba.cut(string))\n",
    "ALL_TOKENS = cut(TEXT)\n",
    "len(ALL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valida_tokens = [t for t in ALL_TOKENS if t.strip() and t != '\\n'] #删空格\n",
    "len(valida_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valida_tokens_1 = [t for t in ALL_TOKENS if not re.findall(' +', t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valida_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter([1,2,3,4,1,2,3,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(valida_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = [f for w,f in word_count.most_common(100)]\n",
    "x = [i for i in range(len(frequencies))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.plot(x, np.log(frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_all = [f for w,f in word_count.most_common()]\n",
    "frequencies_sum = sum(frequencies_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如何解决没见过的单词？\n",
    "假设全部的单词是已知的单词数目在加上现有单词的种类个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smooth(counter,c=1):\n",
    "    N = sum(counter.values())\n",
    "    Nplus = N + c*(len(counter)+1)\n",
    "    \n",
    "    return lambda word : (counter[word]+c)/Nplus    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_prob = laplace_smooth(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_prob(\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_prob(\"我们\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看一下我们所以单词中出现次数是1的单词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_words = [w for w in word_count if word_count[w] == 1]\n",
    "len(single_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些单词的长度分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = list(map(len,single_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(lengths).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考 Good-Turing 的方法（但不是全部实现），我们使用出现次数是1的单词，重新评估了单词的概率，而且还考虑到了单词长度的影响。\n",
    "\n",
    "其中的参数:\n",
    "\n",
    "base: 值得是若单词长度过长，则概率衰减的速度 \n",
    "\n",
    "prior: 指的是出现次数是0的单词比1的单词少的估计倍数，这个单词我们可以自己调节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_good_tuning_version(counter,base=1/26,prior = 1e-8):\n",
    "    N = sum(counter.values())\n",
    "    lengths = map(len,[w for w in counter if counter[w]==1])\n",
    "    ones = Counter(lengths)\n",
    "    longest = max(ones)\n",
    "    \n",
    "    def _get_prob(w):\n",
    "        if w in counter:\n",
    "            prob = counter[w]/N\n",
    "        elif len(w) in ones:\n",
    "            prob = prior*ones[len(w)]/N\n",
    "        else:\n",
    "            prob = prior*ones[longest]/N * base **(len(word)-longest)\n",
    "        \n",
    "        return prob\n",
    "    return _get_prob\n",
    "gt_prob = simple_good_tuning_version(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_prob(\"我们\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_prob(\"啊啊啊啊啊我\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(word):\n",
    "    esp = 1/frequencies_sum\n",
    "    if word in word_count:\n",
    "        return word_count[word]/frequencies_sum\n",
    "    else:\n",
    "        return esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prob(\"我们\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def product(numbers):\n",
    "    return reduce(lambda n1,n2:n1*n2,numbers)\n",
    "product([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_one_gram(string):\n",
    "    words = cut(string)\n",
    "    return product([get_prob(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_one_gram('广交会下个月举办')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_one_gram('一个掉在了民房上')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"\n",
    "这是一个比较正常的句子\n",
    "这个一个比较罕见的句子\n",
    "小明毕业于清华大学\n",
    "小明毕业于秦华大学\n",
    "\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    print(s,language_model_one_gram(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"我去吃火锅，今晚 今晚我去吃火锅\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = language_model_one_gram(s1), language_model_one_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Gram\n",
    "Get the combination probability of 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tokens = [str(t) for t in valida_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_2_gram_words = [''.join(valid_tokens[i:i+2]) for i in range(len(valid_tokens[:-2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2_gram_sum = len(all_2_gram_words)\n",
    "_2_gram_counter = Counter(all_2_gram_words)\n",
    "\n",
    "def get_combination_prob(w1,w2):\n",
    "    if w1+w2 in _2_gram_counter: return _2_gram_counter[w1+w2]/_2_gram_sum\n",
    "    else:\n",
    "        return 1/_2_gram_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_combination_prob('波音', '飞机')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_combination_prob('波音', '北京')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_2_gram(w1,w2):\n",
    "    return get_combination_prob(w1,w2)/get_prob(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prob_2_gram('去', '北京')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prob_2_gram('去', '沈阳')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_of_2_gram(sentence):\n",
    "    sentence_prob = 1\n",
    "    words = cut(sentence)\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        if i == 0:\n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous=words[i-1]\n",
    "            prob = get_prob_2_gram(previous,word)\n",
    "            sentence_prob*=prob\n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_of_2_gram('小明今天抽奖抽到一台苹果手机')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = language_model_of_2_gram(s1), language_model_of_2_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review the main points of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Wikipedia dataset to finish the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import wget\n",
    "url = 'https://dumps.wikimedia.org/zhwiki/20190401/zhwiki-20190401-pages-articles-multistream1.xml-p1p162886.bz2'\n",
    "wget.download(url)\n",
    "use WikiExtractor to compilr all artucles into a list\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/quark30/Desktop/AA/wiki_00.txt\"\n",
    "import glob\n",
    "#The 'glob' library contains a function, also called 'glob', \n",
    "# that finds files and directories whose names match a pattern. \n",
    "files = glob.glob(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_for_assign_3 = []\n",
    "for name in files:\n",
    "    try:\n",
    "        text_file = open(name, \"r\", encoding=\"utf8\")\n",
    "        lines = text_file.readlines()\n",
    "        all_articles_for_assign_3.append(lines)\n",
    "    except IOError as exc: # not sure what error this is\n",
    "        if exc.errno != errno.EISDIR:\n",
    "            raise\n",
    "all_articles_for_assign_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据清理 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = ''\n",
    "for articles in all_articles_for_assign_3:\n",
    "    for element in articles:\n",
    "        if element.startswith('<doc id') or element.startswith('</doc>'):\n",
    "            articles.remove(element)\n",
    "        else:\n",
    "            TEXT += element\n",
    "TEXT.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- remove all punctuation marks and special characters\n",
    "import re\n",
    "def find_all_tokens(string):\n",
    "    \"Define a procedure to find all tokens and removing speciall characters\"\n",
    "    return ' '.join(re.findall('[\\w|\\d]+', string))\n",
    "\n",
    "TEXT_TOKENS = find_all_tokens(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- change all characters into simplified chinese ('hanziconv' package)\n",
    "from hanziconv import HanziConv\n",
    "TEXT_TOKENS = HanziConv.toSimplified(TEXT_TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def cut(string): return list(jieba.cut(string))\n",
    "\n",
    "ALL_TOKENS = cut(TEXT_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- remove whitespaces \n",
    "valid_tokens = [t for t in ALL_TOKENS if t.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the frequences of word and implement the 1-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter(valid_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequences for all single words (1-grams)\n",
    "frequences_all = [f for w, f in words_count.most_common()]\n",
    "frequences_sum = sum(frequences_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we define a procedure to calculate the probability for a give word\n",
    "def get_prob(word):\n",
    "    eps = 1 / frequences_sum # define a small constant, epsilon\n",
    "    if word in words_count:\n",
    "        return words_count[word] / frequences_sum\n",
    "    else:\n",
    "        return eps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a procedure to calculate the probability of a given sentence\n",
    "# using the simplified 1-gram model\n",
    "from functools import reduce\n",
    "\n",
    "def product(numbers):\n",
    "    \"Return the multiplication product for a list of numbers\"\n",
    "    return reduce(lambda n1, n2: n1 * n2, numbers)\n",
    "\n",
    "def sentence_prob_one_gram(string):\n",
    "    \"Return Pr(S) for a given sentence using the 1-gram model\"\n",
    "    words = cut(string)\n",
    "    return product([get_prob(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the 2-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_2_grams_words = [''.join(valid_tokens[i:i+2]) for i in range(len(valid_tokens[:-2]))]\n",
    "_2_gram_sum = len(all_2_grams_words)\n",
    "_2_gram_counter = Counter(all_2_grams_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- implement the 2-gram model\n",
    "def get_comb_prob(w1,w2):\n",
    "    \"Return Pr(w1 * w2) if w1 + w2 occured in the text corpus\"\n",
    "    if w1+w2 in _2_gram_counter:\n",
    "        return _2_gram_counter[w1+w2]/_2_gram_sum\n",
    "    else:\n",
    "        return 1/_2_gram_sum\n",
    "\n",
    "def get_prob_two_gram(w1,w2):\n",
    "    \"Return Pr(w1 * w2) / Pr(w1) if w1 and w1 + w2 occured in the text corpus\"\n",
    "    return get_comb_prob(w1,w2)/get_prob(w1)\n",
    "    \n",
    "def sentence_prob_two_gram(sentence):\n",
    "    \"Return Pr(S) = Pr(w1) * [Pr(w1 * w2) / Pr(w1)] * ...\"\n",
    "    sentence_prob = 1\n",
    "    words = cut(sentence)\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        if i == 0: prob = get_prob(word)\n",
    "        else: \n",
    "            previous = words[i-1]\n",
    "            prob = get_prob_two_gram(previous,word)\n",
    "        sentence_prob *= prob\n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try a 3-gram model and more sentence pairs\n",
    "Pr(S)=Pr(w1)* Pr(w1w2)/Pr(w1)* Pr(w1w2w3)/Pr(w2w3) * ... * Pr(wn-2wn-1wn)/Pr(wn-1wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3_grams_words = [''.join(valid_tokens[i:i+3]) for i in range(len(valid_tokens[:-3]))]\n",
    "_3_gram_sum = len(all_3_grams_words)\n",
    "_3_gram_counter = Counter(all_3_grams_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_three_comb_prob(w1,w2,w3):\n",
    "    \"Return Pr(w1 * w2 * w3) if w1+w2+w3 occured in the text corpus\"\n",
    "    if w1+w2+w3 in _3_gram_counter:\n",
    "        return _3_gram_counter[w1+w2+w3]/_3_gram_sum\n",
    "    else:\n",
    "        return 1/_3_gram_sum\n",
    "    \n",
    "def get_prob_three_gram(w1,w2,w3):\n",
    "    \"Return Pr(w1 * w2 * w3) / Pr(w2 * w3) if w2+w3 and w1+w2+w3 occured in the text corpus\"\n",
    "    return get_three_comb_prob(w1,w2,w3)/get_comb_prob(w2,w3)\n",
    "\n",
    "def sentence_prob_three_gram(sentence):\n",
    "    \"Return Pr(S) = Pr(w1) * [Pr(w1 * w2) / Pr(w1)] * [Pr(w1 * w2 * w3) / Pr(w2 * w3)] ...\"\n",
    "    sentence_prob = 1\n",
    "    words = cut(sentence)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0: \n",
    "            prob = get_prob(word)\n",
    "        elif i == 1: \n",
    "            previous = words[i-1]\n",
    "            prob = get_prob_two_gram(previous,word)\n",
    "        else:\n",
    "            word_n_2 = words[i-2]\n",
    "            word_n_1 = words[i-1]\n",
    "            prob = get_prob_three_gram(word_n_2,word_n_1,word)\n",
    "        sentence_prob *= prob\n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods?\n",
    "advantage: the probabilit-based model is simpler and more flexible than the pattern-match approach\n",
    "disadvantage: the predicting power of the probability-based model is highly dependent on the text corpus we supply (or, the training data we use). Since any text corpus is limited,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
